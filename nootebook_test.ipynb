{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611375d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee41f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import onnxruntime as ort\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as e:\n",
    "    print(f\"Missing required dependency: {e}\")\n",
    "    print(\"Install with: pip install onnxruntime transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb317cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from models/onnx/pii-redaction-model\n",
      "Loading ONNX model from models/onnx/pii-redaction-model/model_optimized.onnx\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "redactor = SimplePIIRedactor(model_path='models/onnx/pii-redaction-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5f1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "My name is John Doe and my email is john@example.com\n",
      "You can reach me at (123) 456-7890 or visit my website at https://www.johndoe.com.\n",
      "I love programming in Python and my favorite library is NumPy.\n",
      "\n",
      "Redacted: \n",
      "My name is *** and my email is ***\n",
      "You can reach me at *** or visit my website at ***\n",
      "I love programming in Python and my favorite library is NumPy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "english_text = \"\"\"\n",
    "My name is John Doe and my email is john@example.com\n",
    "You can reach me at (123) 456-7890 or visit my website at https://www.johndoe.com.\n",
    "I love programming in Python and my favorite library is NumPy.\n",
    "\"\"\"\n",
    "redacted = redactor.redact(english_text, redaction_token=\"***\")\n",
    "print(f\"Original: {english_text}\")\n",
    "print(f\"Redacted: {redacted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c502c201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hebrew: \n",
      "שם חולה: יוסי כהן\n",
      "תאריך לידה: 01/01/1980\n",
      "מספר טלפון: 050-1234567\n",
      "כתובת: רחוב ירושלים 10, תל אביב\n",
      "תעודת זהות: 123456789\n",
      "הערות: יוסי סובל מאלרגיות למזון מסוים.  \n",
      "יש להימנע ממתן מזון המכיל אגוזים.\n",
      "\n",
      "Redacted Hebrew: \n",
      "שם חולה: ***\n",
      "תאריך לידה: ***\n",
      "מספר טלפון: ***\n",
      "כתובת: ***\n",
      "תעודת זהות: ***\n",
      "הערות: *** מאלרגיות למזון מסוים.  \n",
      "יש להימנע ממתן מזון המכיל אגוזים.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hebrew_text = \"\"\"\n",
    "שם חולה: יוסי כהן\n",
    "תאריך לידה: 01/01/1980\n",
    "מספר טלפון: 050-1234567\n",
    "כתובת: רחוב ירושלים 10, תל אביב\n",
    "תעודת זהות: 123456789\n",
    "הערות: יוסי סובל מאלרגיות למזון מסוים.  \n",
    "יש להימנע ממתן מזון המכיל אגוזים.\n",
    "\"\"\"\n",
    "redacted_hebrew = redactor.redact(hebrew_text, redaction_token=\"***\")\n",
    "print(f\"Original Hebrew: {hebrew_text}\")\n",
    "print(f\"Redacted Hebrew: {redacted_hebrew}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c425ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimplePIIRedactor:\n",
    "    \"\"\"Minimal PII redactor using ONNX model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = \"models/onnx/pii-redaction-model\"):\n",
    "        \"\"\"\n",
    "        Initialize the PII redactor.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to directory containing ONNX model files\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.max_length = 128\n",
    "        \n",
    "        # Load tokenizer\n",
    "        print(f\"Loading tokenizer from {model_path}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load ONNX model\n",
    "        onnx_model_path = os.path.join(model_path, \"model_optimized.onnx\")\n",
    "        if not os.path.exists(onnx_model_path):\n",
    "            onnx_model_path = os.path.join(model_path, \"model.onnx\")\n",
    "        \n",
    "        print(f\"Loading ONNX model from {onnx_model_path}\")\n",
    "        self.session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "        \n",
    "        # Labels\n",
    "        self.id_to_label = {0: \"O\", 1: \"B-PII\", 2: \"I-PII\"}\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def redact(self, text: str, redaction_token: str = \"[REDACTED]\") -> str:\n",
    "        \"\"\"\n",
    "        Redact PII from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            redaction_token: Token to replace PII with\n",
    "            \n",
    "        Returns:\n",
    "            Text with PII redacted\n",
    "        \"\"\"\n",
    "        # Tokenize with offset mapping to preserve character positions\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"np\",\n",
    "            return_token_type_ids=True,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Get offset mapping for token-to-character alignment\n",
    "        offset_mapping = inputs.pop(\"return_offsets_mapping\", None)\n",
    "        if offset_mapping is None:\n",
    "            offset_mapping = inputs.pop(\"offset_mapping\", None)\n",
    "        \n",
    "        # Prepare inputs for ONNX\n",
    "        onnx_inputs = {\n",
    "            \"input_ids\": inputs[\"input_ids\"].astype(np.int64),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].astype(np.int64)\n",
    "        }\n",
    "        \n",
    "        # Note: DistilBERT-based models don't use token_type_ids in ONNX export\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = self.session.run(None, onnx_inputs)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = np.argmax(outputs[0], axis=-1)[0]\n",
    "        \n",
    "        # Find PII entities using offset mapping\n",
    "        entities = []\n",
    "        current_entity_start = None\n",
    "        current_entity_end = None\n",
    "        \n",
    "        for i, (pred, (start, end)) in enumerate(zip(predictions, offset_mapping[0])):\n",
    "            if inputs[\"attention_mask\"][0][i] == 0:  # Padding token\n",
    "                break\n",
    "                \n",
    "            # Skip special tokens (offset mapping is (0, 0) for special tokens)\n",
    "            if start == 0 and end == 0:\n",
    "                continue\n",
    "                \n",
    "            label = self.id_to_label[pred]\n",
    "            \n",
    "            if label == \"B-PII\":  # Beginning of PII\n",
    "                # Save previous entity if exists\n",
    "                if current_entity_start is not None:\n",
    "                    entities.append((current_entity_start, current_entity_end))\n",
    "                # Start new entity\n",
    "                current_entity_start = start\n",
    "                current_entity_end = end\n",
    "            elif label == \"I-PII\":  # Inside PII\n",
    "                # Extend current entity\n",
    "                if current_entity_start is not None:\n",
    "                    current_entity_end = end\n",
    "            else:  # Not PII\n",
    "                # End current entity\n",
    "                if current_entity_start is not None:\n",
    "                    entities.append((current_entity_start, current_entity_end))\n",
    "                    current_entity_start = None\n",
    "                    current_entity_end = None\n",
    "        \n",
    "        # Add final entity if exists\n",
    "        if current_entity_start is not None:\n",
    "            entities.append((current_entity_start, current_entity_end))\n",
    "        \n",
    "        # Apply redaction by replacing character ranges (reverse order to avoid offset issues)\n",
    "        redacted_text = text\n",
    "        for start, end in reversed(entities):\n",
    "            redacted_text = redacted_text[:start] + redaction_token + redacted_text[end:]\n",
    "        \n",
    "        return redacted_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1b32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
