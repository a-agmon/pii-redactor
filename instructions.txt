# PII Redaction Model Fine-tuning Plan

## Overview
This document provides a detailed plan for creating a Python script that fine-tunes a multilingual all-MiniLM-L12-v2 model for PII (Personally Identifiable Information) redaction, with special focus on Hebrew language support.

## Project Structure
```
pii-redaction-project/
├── data/
│   ├── raw/
│   ├── processed/
│   └── synthetic/
├── models/
│   ├── checkpoints/
│   └── onnx/
├── src/
│   ├── dataset_creation.py
│   ├── model_training.py
│   ├── onnx_conversion.py
│   └── inference.py
├── config.yaml
└── requirements.txt
```

## 1. Dependencies and Setup

### Required Libraries
```python
# requirements.txt
transformers==4.36.0
torch==2.1.0
datasets==2.14.0
tokenizers==0.15.0
onnx==1.15.0
onnxruntime==1.16.0
optimum==1.16.0
scikit-learn==1.3.0
numpy==1.24.0
pandas==2.0.0
seqeval==1.2.2
accelerate==0.25.0
faker==20.0.0
hebrew-tokenizer==2.3.0
pyyaml==6.0
tqdm==4.66.0
```

## 2. Configuration File

```yaml
# config.yaml
model:
  base_model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  max_length: 128
  label_all_tokens: false

training:
  batch_size: 16
  learning_rate: 5e-5
  num_epochs: 10
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  
dataset:
  train_size: 10000
  val_size: 2000
  test_size: 1000
  languages: ["en", "he", "es", "fr", "de"]
  hebrew_ratio: 0.4  # 40% of dataset will be Hebrew

pii_types:
  - NAME
  - ID_NUMBER
  - PHONE
  - EMAIL
  - ADDRESS
  - CREDIT_CARD
  - DATE_OF_BIRTH
  - PASSPORT
  - BANK_ACCOUNT
  - LICENSE_PLATE

output:
  model_path: "models/checkpoints/pii-redaction-model"
  onnx_path: "models/onnx/pii-redaction-model.onnx"
```

## 3. Dataset Creation Module

### 3.1 Data Structure
The dataset will use BIO (Begin-Inside-Outside) tagging format:
- B-PII: Beginning of a PII entity
- I-PII: Inside a PII entity
- O: Outside (not PII)

### 3.2 Synthetic Data Generation

```python
# dataset_creation.py structure

class HebrewPIIGenerator:
    """Generate synthetic Hebrew PII data"""
    
    def __init__(self):
        self.hebrew_names = [
            "אלון", "שרה", "דוד", "רחל", "משה", "לאה", "יוסף", "מרים",
            "אברהם", "רבקה", "יצחק", "אסתר", "יעקב", "חנה", "נח", "דינה"
        ]
        self.hebrew_surnames = [
            "כהן", "לוי", "מזרחי", "פרץ", "ביטון", "אזולאי", "דהן", "אברהם"
        ]
        self.id_patterns = ["#########"]  # 9 digits for Israeli ID
        
    def generate_templates(self):
        """Generate Hebrew sentence templates with PII"""
        templates = [
            {
                "template": "שמי {NAME} ומספר תעודת הזהות שלי הוא {ID_NUMBER}",
                "entities": ["NAME", "ID_NUMBER"]
            },
            {
                "template": "אני {NAME}, הטלפון שלי {PHONE} והמייל {EMAIL}",
                "entities": ["NAME", "PHONE", "EMAIL"]
            },
            {
                "template": "הכתובת של {NAME} היא {ADDRESS}",
                "entities": ["NAME", "ADDRESS"]
            },
            {
                "template": "נולדתי ב-{DATE_OF_BIRTH}, השם שלי {NAME}",
                "entities": ["DATE_OF_BIRTH", "NAME"]
            },
            {
                "template": "מספר הדרכון של {NAME} הוא {PASSPORT}",
                "entities": ["NAME", "PASSPORT"]
            }
        ]
        return templates

class MultilingualPIIDataset:
    """Create multilingual PII dataset with BIO tags"""
    
    def __init__(self, config):
        self.config = config
        self.hebrew_generator = HebrewPIIGenerator()
        self.faker_generators = {
            'en': Faker('en_US'),
            'es': Faker('es_ES'),
            'fr': Faker('fr_FR'),
            'de': Faker('de_DE')
        }
        
    def generate_sample(self, language='he'):
        """Generate a single training sample"""
        # Returns: {
        #     "text": "שמי אלון ומספר תעודת הזהות שלי הוא 123456789",
        #     "tokens": ["שמי", "אלון", "ומספר", "תעודת", "הזהות", "שלי", "הוא", "123456789"],
        #     "labels": ["O", "B-PII", "O", "O", "O", "O", "O", "B-PII"]
        # }
        
    def create_dataset(self, num_samples):
        """Create complete dataset with train/val/test splits"""
        # Generate samples with proper distribution across languages
        # Ensure Hebrew comprises 40% as specified in config
        # Return Dataset object compatible with HuggingFace
```

### 3.3 Data Processing Pipeline

```python
class PIIDataProcessor:
    """Process and tokenize data for model training"""
    
    def __init__(self, tokenizer, max_length=128):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.label_to_id = {
            "O": 0,
            "B-PII": 1,
            "I-PII": 2
        }
        self.id_to_label = {v: k for k, v in self.label_to_id.items()}
        
    def tokenize_and_align_labels(self, examples):
        """Tokenize text and align labels with subword tokens"""
        # Handle subword tokenization
        # Ensure labels are properly aligned
        # Return tokenized inputs with attention masks and labels
```

## 4. Model Training Module

### 4.1 Model Architecture
```python
# model_training.py

from transformers import AutoModelForTokenClassification, AutoTokenizer
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorForTokenClassification

class PIIRedactionModel:
    """Fine-tune multilingual model for PII detection"""
    
    def __init__(self, config):
        self.config = config
        self.num_labels = 3  # O, B-PII, I-PII
        
        # Load base model
        self.model = AutoModelForTokenClassification.from_pretrained(
            config['model']['base_model'],
            num_labels=self.num_labels,
            ignore_mismatched_sizes=True
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            config['model']['base_model']
        )
        
    def setup_training_args(self):
        """Configure training arguments"""
        return TrainingArguments(
            output_dir=self.config['output']['model_path'],
            evaluation_strategy="epoch",
            save_strategy="epoch",
            learning_rate=self.config['training']['learning_rate'],
            per_device_train_batch_size=self.config['training']['batch_size'],
            per_device_eval_batch_size=self.config['training']['batch_size'],
            num_train_epochs=self.config['training']['num_epochs'],
            weight_decay=self.config['training']['weight_decay'],
            warmup_steps=self.config['training']['warmup_steps'],
            logging_dir='./logs',
            logging_steps=10,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            push_to_hub=False,
            gradient_accumulation_steps=self.config['training']['gradient_accumulation_steps'],
            fp16=True,  # Enable mixed precision training
        )
```

### 4.2 Custom Metrics
```python
def compute_metrics(eval_pred):
    """Compute F1, precision, recall for token classification"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=2)
    
    # Remove padding and compute metrics
    true_labels = []
    true_predictions = []
    
    for prediction, label in zip(predictions, labels):
        true_labels.append([id_to_label[l] for l in label if l != -100])
        true_predictions.append([id_to_label[p] for p, l in zip(prediction, label) if l != -100])
    
    results = seqeval.metrics.classification_report(
        true_labels, true_predictions, output_dict=True
    )
    
    return {
        "precision": results["weighted avg"]["precision"],
        "recall": results["weighted avg"]["recall"],
        "f1": results["weighted avg"]["f1-score"],
    }
```

## 5. ONNX Conversion Module

```python
# onnx_conversion.py

from optimum.onnxruntime import ORTModelForTokenClassification
from transformers import AutoTokenizer

class ONNXConverter:
    """Convert trained model to ONNX format"""
    
    def __init__(self, model_path, onnx_path):
        self.model_path = model_path
        self.onnx_path = onnx_path
        
    def convert(self):
        """Convert PyTorch model to ONNX"""
        # Load the fine-tuned model
        model = ORTModelForTokenClassification.from_pretrained(
            self.model_path,
            export=True
        )
        
        # Save ONNX model
        model.save_pretrained(self.onnx_path)
        
        # Save tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        tokenizer.save_pretrained(self.onnx_path)
        
    def optimize_onnx(self):
        """Optimize ONNX model for inference"""
        # Quantization options
        # Graph optimization
        # Runtime optimization settings
```

## 6. Inference Module

```python
# inference.py

import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np

class PIIRedactor:
    """Inference class for PII redaction using ONNX model"""
    
    def __init__(self, onnx_path):
        # Load ONNX model
        self.session = ort.InferenceSession(
            f"{onnx_path}/model.onnx",
            providers=['CPUExecutionProvider']
        )
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(onnx_path)
        
        self.label_map = {0: "O", 1: "B-PII", 2: "I-PII"}
        
    def predict(self, text):
        """Predict PII entities in text"""
        # Tokenize input
        inputs = self.tokenizer(
            text,
            return_tensors="np",
            padding=True,
            truncation=True,
            max_length=128
        )
        
        # Run inference
        outputs = self.session.run(
            None,
            {
                "input_ids": inputs["input_ids"],
                "attention_mask": inputs["attention_mask"]
            }
        )
        
        # Process predictions
        predictions = np.argmax(outputs[0], axis=-1)
        tokens = self.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        
        # Align predictions with original text
        return self._align_predictions(text, tokens, predictions[0])
        
    def redact(self, text):
        """Redact PII from text"""
        predictions = self.predict(text)
        
        # Example implementation for Hebrew text
        # Input: "שמי אלון ומספר תעודת הזהות שלי הוא 123456789"
        # Output: "שמי [REDACTED] ומספר תעודת הזהות שלי הוא [REDACTED]"
        
        redacted_text = text
        offset = 0
        
        for entity in predictions:
            if entity['label'] != 'O':
                start = entity['start'] + offset
                end = entity['end'] + offset
                redacted_text = (
                    redacted_text[:start] + 
                    "[REDACTED]" + 
                    redacted_text[end:]
                )
                offset += len("[REDACTED]") - (end - start)
                
        return redacted_text
```

## 7. Main Training Script Structure

```python
# main.py

def main():
    # 1. Load configuration
    config = load_config('config.yaml')
    
    # 2. Create synthetic dataset
    dataset_creator = MultilingualPIIDataset(config)
    train_dataset, val_dataset, test_dataset = dataset_creator.create_dataset(
        config['dataset']['train_size']
    )
    
    # 3. Initialize model and tokenizer
    model = PIIRedactionModel(config)
    
    # 4. Process datasets
    processor = PIIDataProcessor(model.tokenizer)
    train_dataset = train_dataset.map(processor.tokenize_and_align_labels, batched=True)
    val_dataset = val_dataset.map(processor.tokenize_and_align_labels, batched=True)
    
    # 5. Setup data collator
    data_collator = DataCollatorForTokenClassification(
        tokenizer=model.tokenizer,
        pad_to_multiple_of=8
    )
    
    # 6. Train model
    trainer = Trainer(
        model=model.model,
        args=model.setup_training_args(),
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=model.tokenizer,
        compute_metrics=compute_metrics,
    )
    
    trainer.train()
    
    # 7. Save model
    trainer.save_model(config['output']['model_path'])
    
    # 8. Convert to ONNX
    converter = ONNXConverter(
        config['output']['model_path'],
        config['output']['onnx_path']
    )
    converter.convert()
    
    # 9. Test inference
    redactor = PIIRedactor(config['output']['onnx_path'])
    test_text = "שמי אלון ומספר תעודת הזהות שלי הוא 123456789"
    print(f"Original: {test_text}")
    print(f"Redacted: {redactor.redact(test_text)}")

if __name__ == "__main__":
    main()
```

## 8. Implementation Notes

### 8.1 Hebrew-Specific Considerations
- Use proper Hebrew tokenization (consider using hebrew-tokenizer library)
- Handle RTL (right-to-left) text properly
- Ensure proper Unicode handling
- Consider Hebrew-specific PII patterns (Israeli ID format, phone numbers)

### 8.2 Performance Optimization
- Use mixed precision training (fp16) to speed up training
- Implement gradient accumulation for larger effective batch sizes
- Use DataLoader with multiple workers
- Consider model pruning for ONNX optimization

### 8.3 Evaluation Strategy
- Implement per-language evaluation metrics
- Track false positive/negative rates for each PII type
- Create confusion matrix for different entity types
- Implement A/B testing framework for model comparison

### 8.4 Production Considerations
- Add logging and monitoring
- Implement batch inference for efficiency
- Add API endpoint wrapper
- Create Docker container for deployment
- Implement model versioning

## 9. Example Usage After Implementation

```python
# Example usage
from inference import PIIRedactor

# Load the ONNX model
redactor = PIIRedactor("models/onnx/pii-redaction-model")

# Hebrew example
hebrew_text = "שמי אלון ומספר תעודת הזהות שלי הוא 123456789"
redacted_hebrew = redactor.redact(hebrew_text)
print(redacted_hebrew)
# Output: "שמי [REDACTED] ומספר תעודת הזהות שלי הוא [REDACTED]"

# English example
english_text = "My name is John Doe and my SSN is 123-45-6789"
redacted_english = redactor.redact(english_text)
print(redacted_english)
# Output: "My name is [REDACTED] and my SSN is [REDACTED]"

# Batch processing
texts = [
    "אני גר ברחוב הרצל 15, תל אביב",
    "הטלפון שלי 050-1234567",
    "המייל שלי הוא example@email.com"
]
redacted_texts = [redactor.redact(text) for text in texts]
```

## 10. Testing Strategy

### 10.1 Unit Tests
- Test each PII generator function
- Test tokenization alignment
- Test model predictions on edge cases
- Test ONNX conversion accuracy

### 10.2 Integration Tests
- End-to-end pipeline testing
- Multi-language support verification
- Performance benchmarking
- Memory usage profiling

### 10.3 Edge Cases to Test
- Mixed language text
- Partial PII entities
- Overlapping entities
- Very long texts
- Special characters and emojis
- Numbers that aren't PII